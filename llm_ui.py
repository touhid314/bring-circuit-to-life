'''
this module is to act as a language interface between the user and the program
'''

import time
from llm.llm_model import get_llm_model

def get_llm_response(prompt:str, model_path:str, circuit, show_execution_time:bool = True):

    # lcpp_llm = get_llm_model(model_path=model_path, show_execution_time=show_execution_time)
    
    if(show_execution_time): inference_start_time = time.time()

    ckt_netlist = str(circuit)
    system_instruction = """SYSTEM: You are a circuit. Your netlist is: {ckt_netlist}."""
    prompt = "Write a linear regression in python"

    prompt_template=f''' {system_instruction}

    USER: {prompt}

    ASSISTANT:
    '''

    # response = lcpp_llm(
    #     prompt=prompt_template,
    #     max_tokens=256,
    #     temperature=0.5,
    #     top_p=0.95,
    #     repeat_penalty=1.2,
    #     top_k=50,
    #     stop = ['USER:'], # Dynamic stopping when such token is detected.
    #     echo=True # return the prompt
    # )
    # response = response["choices"][0]["text"]
    # # print(response)

    response = """this is a simulated response generated by the llm.<exec>analyzer.operating_point()</exec> <exec>analyzer.change_element_value('R1', 200)</exec> <exec>analyzer.get_voltage(['1','2'], show_plot=True)</exec>"""

    if(show_execution_time):
        inference_end_time = time.time()
        print(f"Inference completed in {inference_end_time - inference_start_time:.2f} seconds.")

    return response


def process_prompt(prompt:str, model_path:str, circuit, analyzer):
    '''
    arguments:
        circuit - pyspice ckt object

    given a prompt, this function, finds the response to the prompt by an LLM and perform 2 things:
    1.    execute code generated by the prompt 
    2.    return non-code things
    '''
    import re

    response = get_llm_response(prompt, model_path, circuit, show_execution_time=True)

    exec_pattern = re.compile(r'<exec>(.*?)</exec>')
    exec_commands = exec_pattern.findall(response)
    non_exec_response = exec_pattern.sub('', response).strip()

    if len(exec_commands) != 0:
        # execute analysis as per code_str
        import traceback

        for exec_cmd in exec_commands:
            try:
                print(f"executing...: {exec_cmd}")

                exec(f"def dynamic_func():\n    return {exec_cmd}")
                result = locals()['dynamic_func']()

                print(f"return of execution: {result}")
            except Exception as e:
                print(f"llm generated code execution failed. execution code: {exec_cmd}")
                print(f"> Error message: {e}")
                traceback.print_exc()
                print(f"end of error message <")
    return non_exec_response


if __name__ == "__main__":
    prompt = "what are you?"
    model_path = "llm\models\llama-2-13b-chat.ggmlv3.q5_1.bin"

    # circuit
    from PySpice.Spice.Netlist import Circuit
    circuit = Circuit('RC Circuit')
    circuit.V(1, '1', '0', 10)  # DC Voltage Source: 5V between nodes 'in' and ground
    circuit.R(1, '1', '2', 1e3)       # Resistor: 1 kOhm between 'in' and 'node1'
    circuit.C(1, '2', '0', 1e-6) # Capacitor: 1uF between 'node1' and ground

    # create an analyzer object for the circuit and the llm will perform operations by using this analyzer object
    # analyzer object is like the connecting wire between the LLM and the simulator to send instructions
    from analyse import Analyzer
    analyzer = Analyzer(circuit)

    response_text = process_prompt(prompt, model_path, circuit, analyzer)

    print(f"CIRCUIT SAYS: {response_text}")
